---
title: "Lost in Translation: The Algorithmic Gap Between LMs and the Brain"
collection: publications
permalink: /publication/2024-07-05-paper-at
#excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2024-07-05
venue: 'Workshop on Large Language Models and Cognition, ICML, 2024'
paperurl: 'https://arxiv.org/pdf/2407.04680'
#citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
#citation : 'Pascal Jr. Tikeng Notsawo, Hattie Zhou, Mohammad Pezeshki, Irina Rish, Guillaume Dumas. (2023). "Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok." <i>preprint</i>.'
authors : Tommaso Tosato, Pascal Jr. Tikeng Notsawo, Saskia Helbling, Irina Rish, Guillaume Dumas
---
[comment]: <> This paper is about the number 1. The number 2 is left for future work.

Language Models (LMs) have achieved impressive performance on various linguistic tasks, but their relationship to human language processing in the brain remains unclear. This paper examines the gaps and overlaps between LMs and the brain at different levels of analysis, emphasizing the importance of looking beyond input-output behavior to examine and compare the internal processes of these systems. We discuss how insights from neuroscience, such as sparsity, modularity, internal states, and interactive learning, can inform the development of more biologically plausible language models. Furthermore, we explore the role of scaling laws in bridging the gap between LMs and human cognition, highlighting the need for efficiency constraints analogous to those in biological systems. By developing LMs that more closely mimic brain function, we aim to advance both artificial intelligence and our understanding of human cognition.

Paper : [https://arxiv.org/pdf/2407.04680](https://arxiv.org/pdf/2407.04680)