I"Õ<p>A common practice in deep learning is to stop training a model as soon as a sign of overfitting is observed, or when the modelâ€™s generalization capabilities have not improved over a long training period (<em>early stopping</em>). The limits of this practice are now well-known today, since (i) a modelâ€™s performance can improve, deteriorate and then improve again during training (<em>epoch-wise double descent</em>) (ii) a model can generalize several steps after severe overfitting (<em>grokking</em>). Epoch-wise double descent and grokking open the way to new studies concerning the structure of the minimum found by Stochastic Gradient Descent (SGD), and how networks behave in the neighbourhood of SGD training convergence. These phenomena also lead us to rethink our knowledge about the relationship between the model size, data size, initialization, hyperparameters and generalization of neural networks. Beyond just rethinking this relationship, there appears to be a need to be able to identify measures that are easy and cheaper to obtain and that are strongly correlated with generalization since phenomena such as multiple descents can occur at model sizes that are difficult to experiment with, just as grokking often requires models to be trained for a very large number of epochs, making it difficult to construct a phase diagram of generalization covering all the hyperparameters. With the issues mentioned above in mind, weâ€™ve been looking at grokking recently.</p>
:ET