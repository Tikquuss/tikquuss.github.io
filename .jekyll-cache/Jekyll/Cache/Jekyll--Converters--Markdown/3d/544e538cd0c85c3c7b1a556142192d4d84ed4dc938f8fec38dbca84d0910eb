I"«
<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/publication/2023-06-23-paper-at" rel="permalink">Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok
</a>
      
    </h2>
    
    

        
          <!--- <p>Published in <i>preprint</i>, 2023 </p> --->
          <p>, <i>preprint</i>, 2023.</p>
        

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    
      <!--- <p>Download <a href=" https://arxiv.org/abs/2306.13253 "><u>here</u></a></p> --->
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/publication/2022-02-02-paper-at" rel="permalink">Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization for Heterogeneous Representational Coarseness
</a>
      
    </h2>
    
    

        
          <!--- <p>Published in <i>In Thirthy-Seventh AAAI Conference on Artificial Intelligence</i>, 2023 </p> --->
          <p>Dianbo Liu, Alex Lamb, Xu Ji, Pascal Jr. Tikeng Notsawo, Mike Mozer, Yoshua Bengio, Kenji Kawaguchi, <i>In Thirthy-Seventh AAAI Conference on Artificial Intelligence</i>, 2023.</p>
        

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    
      <!--- <p>Download <a href=" https://arxiv.org/pdf/2202.01334.pdf "><u>here</u></a></p> --->
    

  </article>
</div>

<div class="list__item">
  <article class="archive__item" itemscope="" itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="http://localhost:4000/publication/2021-12-15-paper-at" rel="permalink">On the use of linguistic similarities to improve Neural Machine Translation for African Languages
</a>
      
    </h2>
    
    

        
          <!--- <p>Published in <i>5th Black in AI Workshop @ NeurIPS</i>, 2021 </p> --->
          <p>Pascal Jr. Tikeng Notsawo, Brice Nanda, James Assiene, <i>5th Black in AI Workshop @ NeurIPS</i>, 2021.</p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In recent years, there has been a resurgence in research on empirical methods for machine translation. Most of this research has been focused on high-resource, European languages. Despite the fact that around 30% of all languages spoken worldwide are African, the latter have been heavily under investigated and this, partly due to the lack of public parallel corpora online. Furthermore, despite their large number (more than 2,000) and the similarities between them, there is currently no publicly available study on how to use this multilingualism (and associated similarities) to improve machine translation systems performance on African languages. So as to address these issues: 
We propose a new dataset for African languages that provides parallel data for vernaculars not present in commonly used dataset like JW300. To exploit multilingualism, we first use a historical approach based on historical origins of these languages, their morphologies, their geographical and cultural distributions as well as migrations of population to identify similar vernaculars. We also  propose a new metric to automatically evaluate similarities between languages. This new metric does not require word level parallelism like traditional methods but only paragraph level parallelism.
We then show that performing Masked Language Modelling and Translation Language Modeling in addition to multi-task learning on a cluster of similar languages leads to a strong boost of performance in translating individual pairs inside this cluster. In particular, we record an improvement of 29 BLEU on the pair Bafia-Ewondo using our approaches compared to previous work methods that did not exploit multilingualism in any way.</p>
</p>
    
    
    
      <!--- <p>Download <a href=" https://openreview.net/pdf?id=Q5ZxoD2LqcI "><u>here</u></a></p> --->
    

  </article>
</div>

:ET