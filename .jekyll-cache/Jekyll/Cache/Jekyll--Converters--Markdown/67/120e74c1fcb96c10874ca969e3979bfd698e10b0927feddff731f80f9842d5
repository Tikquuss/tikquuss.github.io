I")<h1 id="tensorized-and-parallelizable-pytorch-implementation-of-the-algorithm-for-intrinsic-dimension-estimation">Tensorized (and parallelizable) pytorch implementation of the algorithm for intrinsic dimension estimation</h1>

<h2 id="1-maximum-likelihood-estimation-appoach">1. Maximum Likelihood Estimation appoach</h2>

<p>Calculates intrinsic dimension of the provided data points with the Maximum Likelihood Estimation appoach.</p>

<p>References:</p>

<ul>
  <li>[1] Elizaveta Levina and Peter J Bickel. Maximum likelihood estimation of intrinsic dimension.  In Advances in neural information processing systems, pp. 777–784, 2005. https://www.stat.berkeley.edu/~bickel/mldim.pdf</li>
  <li>[2] David J.C. MacKay and Zoubin Ghahramani. Comments on ‘maximum likelihood estimation of intrinsic dimension’  by e. levina and p. bickel (2004), 2005. http://www.inference.org.uk/mackay/dimension/</li>
  <li>[3] THE INTRINSIC DIMENSION OF IMAGES AND ITS IMPACT ON LEARNING, Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, Tom Goldstein. https://openreview.net/pdf?id=XJk19XzGq2J</li>
</ul>

<p>One of the main approaches to intrinsic dimension estimation is to examine a neighborhood around each point in the dataset, and compute the Euclidean distance to the $k^{th}$ nearest neighbor. Assuming that density is constant within small neighborhoods, the Maximum Likelihood Estimation (MLE) of [1] uses a Poisson process to model the number of points found by random sampling within a given radius around each sample point. By relating the rate of this process to the surface area of the sphere, the likelihood equations yield an estimate of the ID at a given point $x$ as: 
\(\hat{m}_k(x) = \bigg[ \frac{1}{k-1} \sum_{j=1}^{k-1} log \frac{T_k(x)}{T_j(x)} \bigg]^{-1}\)<br />
where $T_j(x)$ is the Euclidean ($l_2$) distance from $x$ to its $j^{th}$ nearest neighbor. [1] propose to average the local estimates at each point to obtain a global estimate ($n$ is the number of sample) 
\(\hat{m}_k = \frac{1}{n} \sum_{i=1}^{n} \hat{m}_k(x_i)\) 
[2] suggestion a correction based on averaging of inverses 
\(\hat{m}_k = \bigg[ \frac{1}{n} \sum_{i=1}^{n} \hat{m}_k(x_i)^{-1} \bigg]^{-1} = \bigg[ \frac{1}{n(k-1)} \sum_{i=1}^{n} \sum_{j=1}^{k-1} log \frac{T_k(x_i)}{T_j(x_i)} \bigg]^{-1}\)</p>

<h2 id="2-two-nn">2. TWO-NN</h2>
<p>Calculates intrinsic dimension of the provided data points with the TWO-NN algorithm.</p>

<p>References:</p>
<ul>
  <li>E. Facco, M. d’Errico, A. Rodriguez &amp; A. Laio. Estimating the intrinsic dimension of datasets by a minimal neighborhood information. https://doi.org/10.1038/s41598-017-11873-y</li>
</ul>

<p>2-NN algorithm :</p>
<ol>
  <li>Compute the pairwise distances for each point in the dataset $i = 1, …, N$.</li>
  <li>For each point $i$ find the two shortest distances $r_1$ and $r_2$.</li>
  <li>For each point $i$ compute $µ_i = \frac{r_2}{r_1}$</li>
  <li>Compute the empirical cumulate $F^{emp}(μ)$ by sorting the values of $μ$ in an ascending order through a permutation $σ$, then define $F^{emp}(μ_{σ(i)}) = \frac{1}{N}$</li>
  <li>Fit the points of the plane given by coordinates ${(log(μ_i), −log(1−F^{emp}(μ_i)))  i=1,…, N}$ with a straight line passing through the origin.</li>
</ol>

<h2 id="3-installation">3. Installation</h2>
<p>pip install intrinsics_dimension</p>

<h2 id="4-get-started">4. Get started</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="nn">intrinsics_dimension</span> <span class="kn">import</span> <span class="n">mle_id</span><span class="p">,</span> <span class="n">twonn_numpy</span><span class="p">,</span> <span class="n">twonn_pytorch</span>

<span class="n">n</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">1024</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<span class="n">d1</span> <span class="o">=</span> <span class="n">mle_id</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">averaging_of_inverses</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">d2</span> <span class="o">=</span> <span class="n">mle_id</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">averaging_of_inverses</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">d3</span> <span class="o">=</span> <span class="n">twonn_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">return_xy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">d4</span> <span class="o">=</span> <span class="n">twonn_pytorch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_xy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">d3</span><span class="p">,</span> <span class="n">d4</span><span class="p">)</span>

<span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># make distance(data[1], data[0]) = 0
</span><span class="n">d1</span> <span class="o">=</span> <span class="n">mle_id</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">averaging_of_inverses</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
<span class="n">d2</span> <span class="o">=</span> <span class="n">mle_id</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">averaging_of_inverses</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">d3</span> <span class="o">=</span> <span class="n">twonn_numpy</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">return_xy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">d4</span> <span class="o">=</span> <span class="n">twonn_pytorch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">return_xy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">d3</span><span class="p">,</span> <span class="n">d4</span><span class="p">)</span>
</code></pre></div></div>
:ET